# CB-Core Historical Processing Configuration  
# Maximum resource allocation for historical data processing

# Include Iceberg configuration
include "cb_core_iceberg.conf"

# Override resource settings for maximum throughput
spark.app.name                     CB-Core-Historical-Processing

# Driver configuration (increased for large operations)
spark.driver.memory                6g
spark.driver.cores                 1
spark.driver.maxResultSize         4g

# Executor configuration (maximum allocation)
spark.executor.instances           2
spark.executor.cores               3
spark.executor.memory              10g
spark.executor.memoryOverhead      3g

# Resource allocation summary:
# Driver: 1 core + 6GB = 1 core, 6GB
# Executors: 2 * 3 cores + 2 * 10GB = 6 cores, 20GB
# Total: 7/8 cores, 26/32GB (maximum safe allocation)

# Optimizations for large batch processing
spark.sql.adaptive.enabled                    true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled           true
spark.sql.adaptive.localShuffleReader.enabled true

# Increased parallelism for historical data
spark.sql.shuffle.partitions                  400
spark.default.parallelism                     200

# File processing optimizations
spark.sql.files.maxPartitionBytes            268435456
spark.sql.files.openCostInBytes              8388608
spark.sql.adaptive.advisoryPartitionSizeInBytes    268435456

# Broadcast settings for large joins
spark.sql.autoBroadcastJoinThreshold          200MB

# Checkpoint configuration for long-running jobs
spark.cleaner.ttl                             3600s
spark.cleaner.referenceTracking.cleanCheckpoints    true

# Historical data specific settings
spark.serializer.objectStreamReset           100
spark.kryoserializer.buffer.max               1g

# Iceberg settings for historical loads
spark.sql.iceberg.split-size                 268435456
spark.sql.iceberg.planning.preserve-data-grouping    true
